---
title: "Homework 8"
author: "Ron Cordell, Lei Yang, Subhashini Raghunathan"
date: "Apr 7, 2016"
output: pdf_document
---


**Build an univariate linear time series model (i.e AR, MA, and ARMA models) using the series in hw08_series.csv.**
**Use all the techniques that have been taught so far to build the model, including date examination, data visualization, etc.**

```{r}
library(astsa)    # Time series package by Shummway and Stoffer
library(zoo)      # time series package
library(forecast)
library(quantmod) 
setwd("C:/Subha/WS271-Regression/Labs/Data")

data = read.csv("hw08_series.csv")
data.ts = ts(data=data$x)


# 1. Examining the Data

str(data)
summary(data)
head(data, 10)

# 2. Data Visualization
par(mfrow=c(2,2))
plot.ts(data.ts, main="Hw08 series",
        col="blue")
hist(data$x, col="gray", main="Hw08 series")
acf(data.ts, main="ACF of Hw08 series")
pacf(data.ts, main="PACF of Hw08 series")
```

The CSV file for the HW8 time series consists of two variables: an X variable that is the time interval and an x value corresponding to the time period. There is no information about the time interval or units of the values.

The time series plot reveals that the HW8 time series is a persistently upward trending series and is not stationary. The autocorrelation shows a very long decay over more than 25 lags while the partial autocorrelation shows statistically significant results at lags 13 and 25, indicating a strong seasonal component that happens every 12 periods, in addition to the inter-period seasonality.

The ACF gradually decaying and PACF immediately dropping off is indicative of a random walk series.

Given this, it is clear that AR and MA and ARMA models are insufficient to model this series. Still, let's try it.


**All the steps to support your final model need to be shown clearly.**
**Show that the assumptions underlying the model are valid.**
**Which model seems most reasonable in terms of satisfying the model's underling assumption?**



```{r}

#first, let's try several MA models

best_aic_ma = 10000
best_order_ma = 999
all_aics_ma = list(0)

for(i in c(1,5,10,15,20,25,30)) {
  data.fit <- arima(data.ts, order=c(0,0,i))
  if(data.fit$aic < best_aic_ma ){
    best_aic_ma = data.fit$aic
    best_order_ma = length(data.fit$coef) -1
    best_model_ma = data.fit
  }
  all_aics_ma = cbind(all_aics_ma, data.fit$aic)
  
  
}

#the AIC keep reducing; order 30 is the best model
all_aics_ma
best_order_ma
best_model_ma

```

We can see that the AIC keeps reducing as the order increases. For practical considerations we stopped at order 30.

```{r}


#now let's try AR models. Here we find that after order 3, the series 
#cannot be estimated because of non-stationarity.
best_aic_ar = 10000
best_order_ar = 999
all_aics_ar = vector("list", 3)


for(i in 1:3) {
  data.fit <- arima(data.ts, order=c(i,0,0))
  if(data.fit$aic < best_aic_ar ){
    best_aic_ar = data.fit$aic
    best_order_ar = length(data.fit$coef) -1
    best_model_ar = data.fit
  }
  all_aics_ar[i] = data.fit$aic
}

#AICS are quite similar for all 3
all_aics_ar
best_order_ar
best_model_ar


#for arma, (1,1) and (2,1) are the the only models that R will estimate 
#because of non-stationarity in higher models.
best_aic_arma = 10000
best_order_arma = 999
all_aics_arma = vector("list", 2)


for(i in 1:2) {
  if(i == 1)
    data.fit <- arima(data.ts, order=c(1,0,1))
  else
    data.fit <- arima(data.ts, order=c(2,0,1))
  if(data.fit$aic < best_aic_arma ){
    best_aic_arma = data.fit$aic
    best_order_arma = length(data.fit$coef) -1
    best_model_arma = data.fit
  }
  all_aics_arma[i] = data.fit$aic
}

all_aics_arma
best_order_arma
best_model_arma


```

Stationarity Assumptions:

- An MA(q) process is stationary, so the underlying assumptions are satisfied if we select the MA(30) model.

- The assumptions for the AR(p) model are that the series being modeled is stationary. As we have seen, R does not allow us to estimate AR models for this series where p > 3 because it detects that these models are non-stationary.

- The roots of the characteristic polyomial for AR(3) are calculated as follows:

```{r}

#we know that the best AR model has order 3.
polyroot(c(best_model_ar$coef[1], best_model_ar$coef[1], best_model_ar$coef[1]))

```

The roots of this equation are outside the unit circle in absolute value, hence the estimated process is stationary.


Based on the results above, it would seem that MA(30) is the best model to pick. It's AIC is 1553. In contrast, the best AR model AR(3) has AIC of 1787.


**Evaluate the model performance (both in- and out-of-sample)**


We evaluate the MA(30) model here.

```{r}

#model diagnostics - residuals
par(mfrow=c(2,2))
plot.ts(best_model_ma$resid, main="Residual Series",
        ylab="residuals", col="navy")
hist(best_model_ma$resid, col="gray", main="Residuals")
acf(best_model_ma$resid, main="ACF of Residuals")
pacf(best_model_ma$resid, main="PACF of Residuals")

#analysis of residuals
head(cbind(data.ts, fitted(best_model_ma), best_model_ma$resid),10)

df<-data.frame(cbind(data.ts, fitted(best_model_ma), best_model_ma$resid))
library(stargazer)
stargazer(df, type="text")
summary(best_model_ma$resid)

```

These residuals show a definite trend in that they become more volatile and larger over time. The distribution is almost normal. The autocorrelation shows correlations at lags 12 and 24 while the partial autocorrelation shows statistically significant effects at lag 2, 9-14 and 24. These indicate that the seasonality component remains and the series is not stationary.


```{r}
# Model Performance Evaluation Using In-Sample Fit
par(mfrow=c(1,1))
  plot(data.ts, col="navy", 
       main="Original vs Estimated Series (MA(30))",
       ylab="Simulated and Estimated Values", lty=2)
  lines(fitted(best_model_ma),col="orange")
  leg.txt <- c("Original Series", "Estimated Series")
  legend("topright", legend=leg.txt, lty=c(2,1), 
         col=c("navy","orange"), bty='n', cex=1)
  
```

- As we can see, the fitted model follows the original pretty closely.

```{r}
# Forecast - out-of-sample fit
best_model_ma.fcast <- forecast.Arima(best_model_ma, 10)
summary(best_model_ma.fcast)

plot(best_model_ma.fcast, main="10-Step Ahead Forecast and Original & Estimated Series",
    xlab="Simulated Time Period", ylab="Original, Estimated, and Forecasted Values",
    xlim=c(), lty=2, col="navy")
lines(fitted(best_model_ma),col="orange")  
leg.txt <- c("Original Series", "Estimated Series")
legend("topleft",legend=leg.txt,lty=c(1,1,2),
      col=c("navy","orange"),
      bty='n', cex=1)

```

- From the plot above we can see that the model predits a downward trend with a pretty narrow confidence interval, indicating a strong confidence in the continued downward trend in the future.

Finally, we evaluate the model using backtesting.

```{r}
# fit all but the last 72 periods of the time series
ts1.fit_short <- Arima(data.ts[1:300], order=c(0,0,30))
ts1.fit_short
summary(ts1.fit_short)
ts1.fcast_short <- forecast.Arima(ts1.fit_short, h=72)
length(ts1.fcast_short)
summary(ts1.fcast_short$mean)

par(mfrow=c(1,1))
plot(ts1.fcast_short,
     main='12-Step Ahead Forecast, Original Series and Esitmated Series',
     xlab='', ylab='',
     ylim=c(30,160), xlim=c(0,390), lty=1, col='orange')
par(new=T)
plot.ts(data.ts, col="navy",axes=F,xlim=c(0,390),ylab="", lty=1)
  leg.txt <- c("Original Series", "Forecast series")
  legend("top", legend=leg.txt, lty=1, col=c("navy","orange"),
         bty='n', cex=1)




```

The out-of-series forecast created by estimating a model that omits the final 72 periods of the original time series does not capture the observed values of those final 12 periods. This reduces our confidence in the model.

